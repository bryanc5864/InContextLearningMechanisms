2026-01-31 23:05:03,714 [INFO] Experiment 28: Tokenization Confound Analysis
2026-01-31 23:05:03,714 [INFO] Start: 2026-01-31T23:05:03.714358
2026-01-31 23:05:34,387 [INFO] 
============================================================
2026-01-31 23:05:34,387 [INFO] Part A: Per-Task Output Tokenization
2026-01-31 23:05:34,387 [INFO] ============================================================
2026-01-31 23:05:34,493 [INFO] 
  uppercase:
2026-01-31 23:05:34,493 [INFO]     Output token counts: [1, 2, 2, 2, 2]
2026-01-31 23:05:34,493 [INFO]     Mean output tokens: 1.8
2026-01-31 23:05:34,494 [INFO]     Example outputs: ['APPLE', 'BRAIN', 'CLOUD']
2026-01-31 23:05:34,507 [INFO]     'APPLE' -> [' APPLE'] (1 tokens)
2026-01-31 23:05:34,507 [INFO]     'BRAIN' -> [' BRA', 'IN'] (2 tokens)
2026-01-31 23:05:34,508 [INFO]     'CLOUD' -> [' C', 'LOUD'] (2 tokens)
2026-01-31 23:05:34,510 [INFO] 
  first_letter:
2026-01-31 23:05:34,511 [INFO]     Output token counts: [1, 1, 1, 1, 1]
2026-01-31 23:05:34,511 [INFO]     Mean output tokens: 1.0
2026-01-31 23:05:34,511 [INFO]     Example outputs: ['a', 'b', 'c']
2026-01-31 23:05:34,511 [INFO]     'a' -> [' a'] (1 tokens)
2026-01-31 23:05:34,511 [INFO]     'b' -> [' b'] (1 tokens)
2026-01-31 23:05:34,511 [INFO]     'c' -> [' c'] (1 tokens)
2026-01-31 23:05:34,514 [INFO] 
  repeat_word:
2026-01-31 23:05:34,514 [INFO]     Output token counts: [2, 2, 2, 2, 2]
2026-01-31 23:05:34,514 [INFO]     Mean output tokens: 2.0
2026-01-31 23:05:34,514 [INFO]     Example outputs: ['apple apple', 'brain brain', 'cloud cloud']
2026-01-31 23:05:34,514 [INFO]     'apple apple' -> [' apple', ' apple'] (2 tokens)
2026-01-31 23:05:34,515 [INFO]     'brain brain' -> [' brain', ' brain'] (2 tokens)
2026-01-31 23:05:34,515 [INFO]     'cloud cloud' -> [' cloud', ' cloud'] (2 tokens)
2026-01-31 23:05:34,517 [INFO] 
  length:
2026-01-31 23:05:34,518 [INFO]     Output token counts: [2, 2, 2, 2, 2]
2026-01-31 23:05:34,518 [INFO]     Mean output tokens: 2.0
2026-01-31 23:05:34,518 [INFO]     Example outputs: ['5', '5', '5']
2026-01-31 23:05:34,518 [INFO]     '5' -> [' ', '5'] (2 tokens)
2026-01-31 23:05:34,518 [INFO]     '5' -> [' ', '5'] (2 tokens)
2026-01-31 23:05:34,518 [INFO]     '5' -> [' ', '5'] (2 tokens)
2026-01-31 23:05:34,521 [INFO] 
  linear_2x:
2026-01-31 23:05:34,521 [INFO]     Output token counts: [2, 2, 2, 2, 2]
2026-01-31 23:05:34,521 [INFO]     Mean output tokens: 2.0
2026-01-31 23:05:34,521 [INFO]     Example outputs: ['6', '10', '14']
2026-01-31 23:05:34,521 [INFO]     '6' -> [' ', '6'] (2 tokens)
2026-01-31 23:05:34,522 [INFO]     '10' -> [' ', '10'] (2 tokens)
2026-01-31 23:05:34,522 [INFO]     '14' -> [' ', '14'] (2 tokens)
2026-01-31 23:05:34,529 [INFO] 
  sentiment:
2026-01-31 23:05:34,529 [INFO]     Output token counts: [1, 1, 1, 1, 1]
2026-01-31 23:05:34,529 [INFO]     Mean output tokens: 1.0
2026-01-31 23:05:34,529 [INFO]     Example outputs: ['positive', 'negative', 'positive']
2026-01-31 23:05:34,529 [INFO]     'positive' -> [' positive'] (1 tokens)
2026-01-31 23:05:34,529 [INFO]     'negative' -> [' negative'] (1 tokens)
2026-01-31 23:05:34,529 [INFO]     'positive' -> [' positive'] (1 tokens)
2026-01-31 23:05:34,537 [INFO] 
  antonym:
2026-01-31 23:05:34,537 [INFO]     Output token counts: [1, 1, 1, 1, 1]
2026-01-31 23:05:34,537 [INFO]     Mean output tokens: 1.0
2026-01-31 23:05:34,537 [INFO]     Example outputs: ['cold', 'small', 'slow']
2026-01-31 23:05:34,538 [INFO]     'cold' -> [' cold'] (1 tokens)
2026-01-31 23:05:34,538 [INFO]     'small' -> [' small'] (1 tokens)
2026-01-31 23:05:34,538 [INFO]     'slow' -> [' slow'] (1 tokens)
2026-01-31 23:05:34,541 [INFO] 
  pattern_completion:
2026-01-31 23:05:34,541 [INFO]     Output token counts: [1, 2, 1, 1, 1]
2026-01-31 23:05:34,541 [INFO]     Mean output tokens: 1.2
2026-01-31 23:05:34,541 [INFO]     Example outputs: ['B', '2', 'Y']
2026-01-31 23:05:34,542 [INFO]     'B' -> [' B'] (1 tokens)
2026-01-31 23:05:34,542 [INFO]     '2' -> [' ', '2'] (2 tokens)
2026-01-31 23:05:34,542 [INFO]     'Y' -> [' Y'] (1 tokens)
2026-01-31 23:05:34,542 [INFO] 
============================================================
2026-01-31 23:05:34,542 [INFO] Part B: Source/Target Tokenization Alignment
2026-01-31 23:05:34,546 [INFO] ============================================================
2026-01-31 23:05:34,551 [INFO] 
  uppercase -> first_letter:
2026-01-31 23:05:34,552 [INFO]     Source total tokens: 50
2026-01-31 23:05:34,552 [INFO]     Target total tokens: 46
2026-01-31 23:05:34,552 [INFO]     Token count diff: 4
2026-01-31 23:05:34,552 [INFO]     Source output tokens/demo: [1, 2, 2, 2, 2]
2026-01-31 23:05:34,552 [INFO]     Target output tokens/demo: [1, 1, 1, 1, 1]
2026-01-31 23:05:34,552 [INFO]     Output token counts match: False
2026-01-31 23:05:34,552 [INFO]     Known transfer rate: 0.0
2026-01-31 23:05:34,557 [INFO] 
  uppercase -> repeat_word:
2026-01-31 23:05:34,557 [INFO]     Source total tokens: 50
2026-01-31 23:05:34,557 [INFO]     Target total tokens: 51
2026-01-31 23:05:34,557 [INFO]     Token count diff: 1
2026-01-31 23:05:34,557 [INFO]     Source output tokens/demo: [1, 2, 2, 2, 2]
2026-01-31 23:05:34,557 [INFO]     Target output tokens/demo: [2, 2, 2, 2, 2]
2026-01-31 23:05:34,557 [INFO]     Output token counts match: False
2026-01-31 23:05:34,557 [INFO]     Known transfer rate: 0.9
2026-01-31 23:05:34,560 [INFO] 
  first_letter -> repeat_word:
2026-01-31 23:05:34,561 [INFO]     Source total tokens: 46
2026-01-31 23:05:34,561 [INFO]     Target total tokens: 51
2026-01-31 23:05:34,561 [INFO]     Token count diff: 5
2026-01-31 23:05:34,561 [INFO]     Source output tokens/demo: [1, 1, 1, 1, 1]
2026-01-31 23:05:34,561 [INFO]     Target output tokens/demo: [2, 2, 2, 2, 2]
2026-01-31 23:05:34,561 [INFO]     Output token counts match: False
2026-01-31 23:05:34,561 [INFO]     Known transfer rate: 0.0
2026-01-31 23:05:34,572 [INFO] 
  uppercase -> sentiment:
2026-01-31 23:05:34,572 [INFO]     Source total tokens: 51
2026-01-31 23:05:34,572 [INFO]     Target total tokens: 47
2026-01-31 23:05:34,572 [INFO]     Token count diff: 4
2026-01-31 23:05:34,572 [INFO]     Source output tokens/demo: [1, 2, 2, 2, 2]
2026-01-31 23:05:34,573 [INFO]     Target output tokens/demo: [1, 1, 1, 1, 1]
2026-01-31 23:05:34,573 [INFO]     Output token counts match: False
2026-01-31 23:05:34,573 [INFO]     Known transfer rate: 0.0
2026-01-31 23:05:34,576 [INFO] 
  linear_2x -> length:
2026-01-31 23:05:34,577 [INFO]     Source total tokens: 56
2026-01-31 23:05:34,577 [INFO]     Target total tokens: 51
2026-01-31 23:05:34,577 [INFO]     Token count diff: 5
2026-01-31 23:05:34,577 [INFO]     Source output tokens/demo: [2, 2, 2, 2, 2]
2026-01-31 23:05:34,577 [INFO]     Target output tokens/demo: [2, 2, 2, 2, 2]
2026-01-31 23:05:34,577 [INFO]     Output token counts match: True
2026-01-31 23:05:34,577 [INFO]     Known transfer rate: 0.0
2026-01-31 23:05:34,580 [INFO] 
  sentiment -> antonym:
2026-01-31 23:05:34,580 [INFO]     Source total tokens: 46
2026-01-31 23:05:34,580 [INFO]     Target total tokens: 46
2026-01-31 23:05:34,580 [INFO]     Token count diff: 0
2026-01-31 23:05:34,580 [INFO]     Source output tokens/demo: [1, 1, 1, 1, 1]
2026-01-31 23:05:34,580 [INFO]     Target output tokens/demo: [1, 1, 1, 1, 1]
2026-01-31 23:05:34,581 [INFO]     Output token counts match: True
2026-01-31 23:05:34,581 [INFO]     Known transfer rate: 0.1
2026-01-31 23:05:34,581 [INFO] 
============================================================
2026-01-31 23:05:34,581 [INFO] Part C: Do Tokenization Properties Predict Transfer?
2026-01-31 23:05:34,581 [INFO] ============================================================
2026-01-31 23:05:34,581 [INFO] 
  Pairs with matching output token counts (2):
2026-01-31 23:05:34,581 [INFO]     linear_2x -> length: transfer=0.0
2026-01-31 23:05:34,581 [INFO]     sentiment -> antonym: transfer=0.1
2026-01-31 23:05:34,581 [INFO] 
  Pairs with NON-matching output token counts (4):
2026-01-31 23:05:34,581 [INFO]     uppercase -> first_letter: transfer=0.0
2026-01-31 23:05:34,581 [INFO]     uppercase -> repeat_word: transfer=0.9
2026-01-31 23:05:34,581 [INFO]     first_letter -> repeat_word: transfer=0.0
2026-01-31 23:05:34,581 [INFO]     uppercase -> sentiment: transfer=0.0
2026-01-31 23:05:34,598 [INFO] 
  Correlation(output_token_diff, transfer_rate): r = -0.3523
2026-01-31 23:05:34,599 [INFO]   Interpretation: Tokenization differences DO NOT explain transfer
2026-01-31 23:05:34,599 [INFO] 
============================================================
2026-01-31 23:05:34,599 [INFO] Part D: Position Mapping in Multi-Position Intervention
2026-01-31 23:05:34,599 [INFO] ============================================================
2026-01-31 23:05:34,599 [INFO] 
  In exp8, source activations are extracted at positions determined by
2026-01-31 23:05:34,599 [INFO]   the SOURCE prompt's tokenization, then injected into the TARGET prompt
2026-01-31 23:05:34,601 [INFO]   at the SAME absolute positions. If source and target have different
2026-01-31 23:05:34,601 [INFO]   numbers of demo tokens, some positions are misaligned.
2026-01-31 23:05:34,601 [INFO] 
  uppercase -> first_letter:
2026-01-31 23:05:34,601 [INFO]     Source demo output positions: 9
2026-01-31 23:05:34,601 [INFO]     Target demo output positions: 5
2026-01-31 23:05:34,601 [INFO]     Overlapping positions: 5
2026-01-31 23:05:34,601 [INFO]     Misaligned positions: 4
2026-01-31 23:05:34,601 [INFO]     Transfer rate: 0.0
2026-01-31 23:05:34,601 [INFO] 
  uppercase -> repeat_word:
2026-01-31 23:05:34,601 [INFO]     Source demo output positions: 9
2026-01-31 23:05:34,601 [INFO]     Target demo output positions: 10
2026-01-31 23:05:34,601 [INFO]     Overlapping positions: 9
2026-01-31 23:05:34,601 [INFO]     Misaligned positions: 1
2026-01-31 23:05:34,601 [INFO]     Transfer rate: 0.9
2026-01-31 23:05:34,601 [INFO] 
  first_letter -> repeat_word:
2026-01-31 23:05:34,601 [INFO]     Source demo output positions: 5
2026-01-31 23:05:34,602 [INFO]     Target demo output positions: 10
2026-01-31 23:05:34,602 [INFO]     Overlapping positions: 5
2026-01-31 23:05:34,602 [INFO]     Misaligned positions: 5
2026-01-31 23:05:34,602 [INFO]     Transfer rate: 0.0
2026-01-31 23:05:34,602 [INFO] 
  uppercase -> sentiment:
2026-01-31 23:05:34,602 [INFO]     Source demo output positions: 9
2026-01-31 23:05:34,602 [INFO]     Target demo output positions: 5
2026-01-31 23:05:34,602 [INFO]     Overlapping positions: 5
2026-01-31 23:05:34,602 [INFO]     Misaligned positions: 4
2026-01-31 23:05:34,602 [INFO]     Transfer rate: 0.0
2026-01-31 23:05:34,602 [INFO] 
  linear_2x -> length:
2026-01-31 23:05:34,603 [INFO]     Source demo output positions: 10
2026-01-31 23:05:34,603 [INFO]     Target demo output positions: 10
2026-01-31 23:05:34,603 [INFO]     Overlapping positions: 10
2026-01-31 23:05:34,603 [INFO]     Misaligned positions: 0
2026-01-31 23:05:34,603 [INFO]     Transfer rate: 0.0
2026-01-31 23:05:34,603 [INFO] 
  sentiment -> antonym:
2026-01-31 23:05:34,603 [INFO]     Source demo output positions: 5
2026-01-31 23:05:34,603 [INFO]     Target demo output positions: 5
2026-01-31 23:05:34,603 [INFO]     Overlapping positions: 5
2026-01-31 23:05:34,603 [INFO]     Misaligned positions: 0
2026-01-31 23:05:34,603 [INFO]     Transfer rate: 0.1
2026-01-31 23:05:34,603 [INFO] 
============================================================
2026-01-31 23:05:34,603 [INFO] SUMMARY: Tokenization Confound Assessment
2026-01-31 23:05:34,603 [INFO] ============================================================
2026-01-31 23:05:34,603 [INFO] 
  1. SINGLE-TOKEN output tasks (first_letter, length, sentiment, antonym,
     pattern_completion) have minimal tokenization confounds. Transfer
     results for these tasks are robust.

  2. MULTI-TOKEN output tasks (uppercase, repeat_word) have consistent
     token counts within each task (all 5-letter words -> same token count).
     Alignment is therefore reliable within these tasks.

  3. CROSS-TASK token count differences exist but do NOT predict transfer:
     - uppercase -> repeat_word has DIFFERENT token counts but 90% transfer
     - uppercase -> sentiment has SIMILAR token counts but 0% transfer
     This rules out tokenization alignment as the primary driver.

  4. The multi-position intervention uses ABSOLUTE positions from the source
     prompt. When source and target have different tokenization, extra
     positions simply fall on non-output tokens. This dilutes but does not
     create spurious transfer signal.

  CONCLUSION: Tokenization confounds do not explain the observed transfer
  patterns. The primary predictor remains structural output format
  compatibility, not token count alignment.

2026-01-31 23:05:34,605 [INFO] 
Results saved to results/exp28
2026-01-31 23:05:34,605 [INFO] Experiment 28 complete: 2026-01-31T23:05:34.605121
